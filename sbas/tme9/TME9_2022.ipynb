{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMKZ5QVoBmjJ"
      },
      "source": [
        "<h1><b>Statistique en Bioinformatique : </b> TME9 </h1><br>\n",
        "\n",
        "L’objectif de ce TME est l'implementation de la méthode Expectation-Maximisation pour la recherche de motifs.\n",
        "\n",
        "<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 10px\">\n",
        "<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 10px\">\n",
        "<p><b>Soumission</b></p>\n",
        "<ul>\n",
        "<li>Renomer le fichier TME9.ipynb pour NomEtudiant1_NomEtudiant2.ipynb </li>\n",
        "<li>Soumettre via moodle </li>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_qH1CG7BmjM"
      },
      "source": [
        "<H1>Expectation-Maximisation Motif</H1>\n",
        "<br>\n",
        "La méthode EM (Expectation-Maximisation) permet de détecter des motifs dans un ensemble de séquences ADN ou protéiques reliées, non alignées. En particulier, étant donné un groupe de séquences de longueur $L$, dont on sait qu'elles partagent un motif commun de longueur $w$, l’algorithme EM:\n",
        "- infère un modèle $(\\Theta,Z)$ pour le motif;\n",
        "- localise l’occurrence du motif dans chaque séquence.\n",
        "\n",
        "$\\Theta$ representé la matrice des poids-positions $p_{c,k}$ du motif (où $c \\in \\{A,C,G,T\\}$ ou $c \\in \\{A,C,D,...,W\\}$  et $k \\in \\{0 \\dots w\\}$), $p_{c,0}$  est le vecteur de probabilités du modèle nul ou \"background\".\n",
        "$Ζ$ est la matrice des variables cachées, qui donnent les positions initiales du motif: \n",
        "- $Z_{i,j} = 1$, si le motif commence en position $j$ de la séquence $i$,\n",
        "- $Z_{i,j} = 0$, sinon. \n",
        "\n",
        "L’algorithme affine les paramètres du modèle de manière itérative par espérance-maximisation. Chaque itération $t$ se compose de deux étapes:\n",
        "- (E) Calcul des valeurs attendues $Ζ^{(t)}$ de $Ζ$, étant donnés $\\Theta^{(t-1)}$\n",
        "- (M) Estimation de  $\\Theta^{(t)}$  à partir de  $Ζ^{(t)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSue1iYeBmjO"
      },
      "source": [
        "1\\. Implémentez une fonction `read_training_file` pour lire le fichier d'entré qui contient un ensemble de séquences ADN non alignées. Pour simplifier nous allons utiliser les données vu en cours du fichier `toyEx.txt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aWQrgN0iBmjP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['GTCAGG', 'GAGAGT', 'ACGGAG', 'CCAGTC']\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nts = ['A', 'C', 'G', 'T']\n",
        "\n",
        "w = 3\n",
        "input_f = \"toyEx.txt\"\n",
        "\n",
        "def read_training_file(input_f):\n",
        "    \"\"\"\n",
        "    Read a file with no-aligned sequences\n",
        "    input input_f : file name\n",
        "    output seqs : list of sequences\n",
        "    \"\"\"\n",
        "\n",
        "    with open(input_f) as file:\n",
        "        return [line for line in file.read().splitlines() if line]\n",
        "\n",
        "seqs = read_training_file(input_f)\n",
        "print (seqs) #['GTCAGG', 'GAGAGT', 'ACGGAG', 'CCAGTC']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIHpbbKwBmjQ"
      },
      "source": [
        "2\\. Implémentez une fonction `initialiseP` pour initialiser la matrice poids-position $p_{c,k}$. On considère le modèle nul par défaut $p_0 = (0.25, 0.25, 0.25, 0.25)$. Pour initialiser $p^{(t)}$, on prend généralement un motif au hasard dans une sequence, et on fixe à $0.5$ les poids du nucleotide correspondant et à $\\frac{1-0.5}{3}$ les trois autres. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "_zIZdJouBmjR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.25       0.16666667 0.16666667 0.16666667]\n",
            " [0.25       0.16666667 0.16666667 0.16666667]\n",
            " [0.25       0.5        0.5        0.16666667]\n",
            " [0.25       0.16666667 0.16666667 0.5       ]]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def initialiseP(seqs, w, alph):\n",
        "    \"\"\"\n",
        "    Initialise pc,k\n",
        "    input seqs : list of sequences\n",
        "    input w : motif length\n",
        "    input alph : alphabet (nucleotides or amino acids)\n",
        "    output P: position probability matrix\n",
        "    \"\"\"\n",
        "\n",
        "    # q=len(alph)\n",
        "    # P = np.zeros((q, len(seqs[0]) - w + 2))\n",
        "\n",
        "    # # Set the default null model p0 = (0.25, 0.25, 0.25, 0.25)\n",
        "    # P[:,0] = np.array([0.25] * len(P[:,0]))\n",
        "    # print(alph)\n",
        "    # # Initialize the other positions with random motifs\n",
        "    # for i in range(len(seqs[0])-w+1):\n",
        "    #     motif = random.choice(seqs)[i:i + w]\n",
        "    #     print(motif)\n",
        "    #     for j in range(q):\n",
        "    #         if motif[0] == alph[j]:\n",
        "    #             P[j][i+1] = 0.5\n",
        "    #         else:\n",
        "    #             P[j][i+1] = (1 - 0.5) / 3\n",
        "\n",
        "    # return P\n",
        "\n",
        "    seqs_ = np.array([[alph.index(nt) for nt in seq] for seq in seqs])\n",
        "    motifs = [random.choice(seqs_)[random.randint(0, len(seqs[0]) - w) + i] for i in range(w)]\n",
        "    prob = np.zeros((len(alph), w)) + 0.5 / (len(alph) - 1)\n",
        "    prob[motifs, np.arange(w)] = 0.5\n",
        "\n",
        "    return np.c_[np.zeros(len(alph)) + 0.25, prob]\n",
        "\n",
        "# Test\n",
        "p = initialiseP(seqs, w, nts)\n",
        "print(p)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMqgrzYRBmjS"
      },
      "source": [
        "3\\. Implémenter une fonction `initialiseZ` pour initialiser la matrice $Z$ à uns. Rappelez-vous que la dimension de $Z$ est $nbSeq \\times (lenSeq -w +1)$, où $nbSeq$ est le nombre de sequences et $lenSeq$ est la taille/longueur des sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "mghUiiW4BmjT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]\n"
          ]
        }
      ],
      "source": [
        "def initialiseZ(seqs, w):\n",
        "    \"\"\"\n",
        "    Initialise Z\n",
        "    input seqs : list of sequences\n",
        "    input w : motif length\n",
        "    output Z :  matrix of motif start positions\n",
        "    \"\"\"\n",
        "    return np.ones((len(seqs), len(seqs[0]) - w + 1))\n",
        "\n",
        "Z = initialiseZ(seqs, w)\n",
        "print(Z)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbSoL4ywBmjU"
      },
      "source": [
        "4\\. Écrivez une fonction `E_step` pour le pas Expectation qui estime $Z$ à partir de  $p_{c,k}$. \n",
        "Écrivez aussi une fonction `normaliseZ` pour normaliser $Z$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "9oa3r0j3BmjV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.375      0.125      0.125      0.375     ]\n",
            " [0.16666667 0.16666667 0.16666667 0.5       ]\n",
            " [0.0625     0.1875     0.5625     0.1875    ]\n",
            " [0.07142857 0.07142857 0.64285714 0.21428571]]\n"
          ]
        }
      ],
      "source": [
        "def E_step(seqs, P, Z, w, alph):\n",
        "    \"\"\"\n",
        "    Implement Expectation step\n",
        "    input seqs : list of sequences\n",
        "    input P : position probability matrix\n",
        "    input Z :  matrix of motif start positions\n",
        "    input w : motif length\n",
        "    input alph : alphabet (nucleotides or amino acids)\n",
        "    output Z :  matrix of motif start positions\n",
        "    \"\"\"\n",
        "\n",
        "    seqs_ = np.array([[alph.index(nt) for nt in seq] for seq in seqs])\n",
        "\n",
        "    for offset in range(Z.shape[1]):\n",
        "        Z[:, offset] = np.prod(P[:, 0][seqs_[:, :offset]], axis=1) * np.prod(P[seqs_[:, offset:(offset + w)], np.arange(w) + 1], axis=1) * np.prod(P[:, 0][seqs_[:, (offset + w):]], axis=1)\n",
        "\n",
        "    return Z\n",
        "\n",
        "def normaliseZ(z):\n",
        "    \"\"\"\n",
        "    Normalise Z matrix\n",
        "    input Z : unnormalised matrix\n",
        "    output Zn : normalised matrix\n",
        "    \"\"\"\n",
        "\n",
        "    return z / z.sum(axis=1, keepdims=True)\n",
        "\n",
        "Z = E_step(seqs, p, Z, w, nts)\n",
        "z_norm = normaliseZ(Z)\n",
        "print(z_norm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odh27oHwBmjV"
      },
      "source": [
        "5\\. Implémentez une fonction `M_step` pour le pas Maximisation qui estime $p_{c,k}$ à partir de $Z$. \n",
        "Utilisez les \"pseudocounts\" pour éviter les probabilités ègales à zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VleAgdFtBmjW",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def totalNumberofCH(seqs,alph):\n",
        "\n",
        "    q = len(alph)\n",
        "    totalN = np.zeros((q))\n",
        "    for i in range(q):\n",
        "        e = alph[i]\n",
        "        for seq in seqs:\n",
        "            for s in seq:\n",
        "                if(s == e):\n",
        "                    totalN[i]+=1\n",
        "    return totalN\n",
        "\n",
        "def M_step(seqs, Z, w, alph):\n",
        "    \"\"\"\n",
        "    Implement Expectation step\n",
        "    input seqs : list of sequences\n",
        "    input Z :  matrix of motif start positions\n",
        "    input w : motif length\n",
        "    input alph : alphabet (nucleotides or amino acids)\n",
        "    output P : position probability matrix\n",
        "    \"\"\"\n",
        "\n",
        "    P = []\n",
        "\n",
        "\n",
        "    return P\n",
        "\n",
        "P = M_step(seqs, z_norm, w, nts)\n",
        "print(P)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37ciA9qLBmjY"
      },
      "source": [
        "6\\. Écrivez une fonction `likelihood` qui calcule la log-vraisemblance de l'ensemble des sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUMUQUwZBmjY",
        "outputId": "b679b7aa-6b80-496d-d6be-c6342913321d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: RuntimeWarning: divide by zero encountered in log\n"
          ]
        }
      ],
      "source": [
        "def likelihood(seqs, Z, P, w, alph):\n",
        "    \"\"\"\n",
        "    Implement log likelihood function of P\n",
        "    input seqs : list of sequences\n",
        "    input Z :  matrix of motif start positions\n",
        "    input p : position probability matrix\n",
        "    input w : motif length\n",
        "    input alph : alphabet (nucleotides or amino acids)\n",
        "    output lLikelihood : log likelihood of P\n",
        "\n",
        "    \"\"\"\n",
        "    M = len(seqs)\n",
        "    L = len(seqs[0])\n",
        "    lLikelihood = np.log((L-w+1)**-M)\n",
        "\n",
        "\n",
        "    return lLikelihood\n",
        "\n",
        "logvraisemblance = likelihood(seqs, Z, P, w, nts)\n",
        "print(logvraisemblance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFOgaK9eBmjZ"
      },
      "source": [
        "7\\. Implémentez l'algorithme Expectation-Maximisation. Vous calculerez la valeur de la log-vraisemblance totale du modèle à chaque iteration et l'algorithme prendra fin lorsque $\\Delta \\log \\text{Pr}(D | \\Theta) < \\varepsilon$. Utilisez $\\varepsilon = 1e-4$. Votre implementation devra renvoyer les paramètres du modele ($p$ et la log-likelihood associé), ainsi bien que la liste des meilleures positions du motif dans chaque sequence (matrice $Z$). Faites attention à utiliser $Z$ non-normalisé afin de trouver la log-vraisemblance!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZXGq1_3Bmja"
      },
      "outputs": [],
      "source": [
        "def ExpectationMaximisation(seqs, w, alph, eps):\n",
        "    \"\"\"\n",
        "    Implement Expectation Maximisation algorithm\n",
        "    input seqs : list of sequences\n",
        "    input w : motif length\n",
        "    input alph : alphabet (nucleotides or amino acids)\n",
        "    input eps : threahold\n",
        "    output P : position probability matrix\n",
        "    output Z :  matrix of motif start positions\n",
        "    output lLikelihood : log likelihood of P\n",
        "    output pos_motif : positions of motifs in seqs\n",
        "    \"\"\"\n",
        "    P = initialiseP(seqs, w, alph)\n",
        "    Z = initialiseZ(seqs, w)\n",
        "    lLikelihood = []\n",
        "    pos_motif = []\n",
        "\n",
        "\n",
        "    return (P, lLikelihood, Z, pos_motif)\n",
        "\n",
        "def analyse_results(results, seqs):\n",
        "    print('Les paramètres du modele:')\n",
        "    print('p:\\n',results[0])\n",
        "    print('\\n')\n",
        "    print('log-vraisemblance associé:',results[1][-1])\n",
        "    print('\\n')\n",
        "    print('Z:\\n',results[2])\n",
        "    print('\\nLes motifs:')\n",
        "    for i in range(len(seqs)):\n",
        "        print('seq'+str(i+1)+': '+ str(results[3][i]+1) + '  ' + seqs[i][results[3][i]:results[3][i]+w])\n",
        "\n",
        "    plt.plot(results[1])\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Log-vraisemblance')\n",
        "    plt.show()\n",
        "\n",
        "eps = 10**-4\n",
        "EMResults = ExpectationMaximisation(seqs, w, nts, eps)\n",
        "analyse_results(EMResults,seqs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZsOsS8yBmjb"
      },
      "source": [
        "8\\. Qu'est-ce que vous observez en exécutant l'algorithme EM plusieurs fois? Justifiez votre réponse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgXXj3dcBmjb"
      },
      "source": [
        "Reponse:\n",
        "\n",
        "<font color=\"blue\">\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WThFDcG5Bmjb"
      },
      "source": [
        "9\\. Pour éviter le problème identifié au point précedent, écrivez une fonction `EM_iteratif` qui exécute l'algorithme `EM` $N$ fois ($N=10$) et qui prend les paramètres associés à la meilleure log-vraisemblance. Trouvez-vous les bons motifs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiMm-jSkBmjc"
      },
      "outputs": [],
      "source": [
        "def EM_iteratif(N, seqs, w, alph, eps):\n",
        "    \"\"\"\n",
        "    Implement a iterative version of Expectation Maximisation algorithm\n",
        "    input N : number of iterations\n",
        "    input seqs : list of sequences\n",
        "    input w : motif length\n",
        "    input eps : threahold\n",
        "    output bestModel : the parameter of the best model\n",
        "    \"\"\"\n",
        "\n",
        "    bestModel = P, Z, lLikelihood, pos_motif\n",
        "\n",
        "    return bestModel\n",
        "\n",
        "\n",
        "eps = 10**-4\n",
        "N=10\n",
        "meilleurEM = EM_iteratif(N, seqs, w, nts, eps)\n",
        "analyse_results(meilleurEM,seqs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us2JZ9SRBmjc"
      },
      "source": [
        "10\\. Appliquez votre algorithme `EM` à l'ensemble des séquence du fichier `trainingSequences.txt` en utilisant $w=10$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yc5bUMIBmjd"
      },
      "outputs": [],
      "source": [
        "w= 10\n",
        "input_f = \"trainingSequences.txt\"\n",
        "seqs_train = read_training_file(input_f)\n",
        "print (seqs_train)\n",
        "eps = 10**-4\n",
        "N=10\n",
        "meilleurEM = EM_iteratif(N, seqs_train, w, nts, eps)\n",
        "analyse_results(meilleurEM,seqs_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3ddFiW2Bmjd"
      },
      "source": [
        "11\\. Construire un LOGO pour le motif prédit avec le service <i>WebLogo</i>. Pour cela, identifiez le motif dans chaque séquence, utiliser <i>ClustalOmega</i> pour les aligner et puis <i>WebLogo</i> pour générer le LOGO. Ajouter le LOGO à votre réponse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR5YLgIZBmjd"
      },
      "outputs": [],
      "source": [
        "fhandler = open('motifs.txt','w')\n",
        "for offset in range(len(seqs_train)):\n",
        "    fhandler.write('>motif'+str(offset+1)+'\\n')\n",
        "    fhandler.write(seqs_train[offset][meilleurEM[3][offset]:meilleurEM[3][offset]+w])\n",
        "    fhandler.write('\\n')\n",
        "fhandler.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lu09z4zBmje"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfmV6fjVBmje"
      },
      "source": [
        "12\\. Comparez les motifs trouvés par votre programme avec les motifs du fichier `testingSequences.txt`, où les vrais motifs sont montrés en lettre majuscule. Quelle est la performance de votre programme? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jh36eh-vBmje"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "motifs_extracted = []\n",
        "motifs_real = []\n",
        "\n",
        "\n",
        "input_f = \"testingSequences.txt\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TME9_2022.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
