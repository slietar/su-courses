{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEA_dgcQ0OwR"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "**Making sure you can run PyTorch code**\n",
        "\n",
        "In our first practical session, we will learn the basics of how to train, test and use simple multilayered perceptron networks.\n",
        "\n",
        "But first we need to verify that we are able to run pyTorch code. you can execute the code blocks as the one below by clicking shift+Enter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpq8peQS0QQI",
        "outputId": "33ca8cbf-2cc2-4932-bb99-74b258ab6e15"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running in Google colab, no need to install libraries')\n",
        "else:\n",
        "  print('Download the requirements.txt file from the course Web site in order to automatically install required libraries on a local machine via pip')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8JUkJ8j00YN"
      },
      "source": [
        "If you ran the above code snippet, you should see that you are either in google colab, or in some other environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUa650MT0qbU",
        "outputId": "3be14d89-5f1a-4a0d-dfab-d8885987340c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "print(f'pytorch version: {torch.__version__}')\n",
        "\n",
        "num_gpu = torch.cuda.device_count()\n",
        "print(f'{num_gpu} GPU available')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjoPMl5j05Vo"
      },
      "source": [
        "If the above code has printed 0 GPU available, please make sure you switched to the T4 GPU capable runtime (top right corner of the page), or else, the exercises will be very slow...\n",
        "\n",
        "Once you are running a runtime with GPU, you can also run a shell command nvidia-smi, to check on the status of your GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjqG11gZ03pw",
        "outputId": "d71382b6-1086-492c-ad31-6bdb348c8afe"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP_FUqTx1B8C"
      },
      "source": [
        "#Example 1 - Simple linear regression task\n",
        "\n",
        "Now that everything is up a running, let us check one of the simplest tasks, a neural network can perform - a linear regression. I.e. we will generate some data that will correspond to an approximate linear function and we will try to train a model that realizes this function.\n",
        "\n",
        "First, let us generate some data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "g4L0pBZC0-6U",
        "outputId": "6a5ea3a5-cadf-4d0c-b09f-40f070cf40e8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "Xs = np.linspace(-20,20,100) # we start with a hundered X values between -20 and 20\n",
        "\n",
        "Ys=Xs*5+3.5 # as our Xs are a numpy array, we can perform simple arithmetic on each element of Xs\n",
        "\n",
        "#Now, we can plot the data we generated\n",
        "\n",
        "plt.plot(Xs,Ys,\"r.\",label=\"training data\")\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgJuEiJS1c9c"
      },
      "source": [
        "Now we need to define the model. We will go with a very simple network, with one input, one output, and a hidden layer with 4 neurons.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCOoDjKq1OIo"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "#we set the parameters of our simple network architecture\n",
        "N_input=1\n",
        "N_hidden=4\n",
        "N_output=1\n",
        "\n",
        "#create the model based on these parameters\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(N_input, N_hidden),\n",
        "    nn.Linear(N_hidden, N_output),\n",
        ")\n",
        "\n",
        "#convert our numpy arrays Xs and Ys into torch vectors we can use in training\n",
        "\n",
        "inputs = torch.tensor(Xs.reshape((100,1)), dtype=torch.float32)\n",
        "targets = torch.tensor(Ys.reshape((100,1)), dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_m4to931lfi"
      },
      "source": [
        "Now that we have the data and the model, we need to specify the optimizer and a loss function. As this is a regression task, we will use the MSE (MeanSquareError) loss function and a simple SGD(stochastic gradient descent optimizer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFeZyCAe1kL6"
      },
      "outputs": [],
      "source": [
        "criterion =  torch.nn.MSELoss()  # Mean Square Error\n",
        "\n",
        "# Construct the optimizer (Stochastic Gradient Descent in this case)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001) # lr is the learning rate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kA9sTpd1wYL"
      },
      "source": [
        "Finally, we need to set up the main training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc5VKDA51tdK",
        "outputId": "d1fa5ffc-7876-40a0-d02d-ebac681657a9"
      },
      "outputs": [],
      "source": [
        "N_epochs=3000\n",
        "loss_vals=[]\n",
        "# Gradient Descent\n",
        "for epoch in range(N_epochs):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(inputs)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, targets)\n",
        "    if epoch % 100 == 0:\n",
        "        print('epoch: ', epoch, ' loss: ', loss.item())\n",
        "        loss_vals.append(loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # perform a backward pass (backpropagation)\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the parameters\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq1Nt-hM2Ci1"
      },
      "source": [
        "After training, we can also plot the evolution of the loss function value over the training period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "tX_iAGqv1yd2",
        "outputId": "7d6d646a-33d9-4882-e360-2c445fc51dd4"
      },
      "outputs": [],
      "source": [
        "plt.plot(loss_vals,label=\"loss over epochs\")\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGr3Ay9SGF_S"
      },
      "source": [
        "As we can see, the first step is the most helpful, but actually, there is more happening than meets the eye in this plot, we just need to explore it in more detail by changing the y-axis range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "GBApN3_2GVOR",
        "outputId": "50b930fd-cbe0-4a2a-8aa6-17716d1e366d"
      },
      "outputs": [],
      "source": [
        "plt.plot(loss_vals,label=\"loss over epochs\")\n",
        "plt.legend()\n",
        "plt.ylim(-.5,15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjVUDLn2NFA"
      },
      "source": [
        "Now that we have trained our model, we can see how it can be used on some new testing data. We will generate a few points inside the interval that the network was trained on and a few samples outside, to see if the network generalizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "VwnrCiCF2JSG",
        "outputId": "04f68044-3842-4a11-ee46-cc1f44f78ae5"
      },
      "outputs": [],
      "source": [
        "test_Xs= np.linspace(-100,100,35)\n",
        "test_Ys=test_Xs*5+3.5\n",
        "\n",
        "with torch.no_grad(): # turning off the autograd of PyTorch\n",
        "    test_data = torch.tensor(test_Xs.reshape((35,1)), dtype=torch.float32)\n",
        "    test_output = model(test_data)\n",
        "\n",
        "#plot the results\n",
        "plt.plot(Xs,Ys,\"r.\",label=\"training data\")\n",
        "plt.plot(test_Xs,test_output,\"k+\",label=\"network testing output\")\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHnLMRJr2bXt"
      },
      "source": [
        "As we can see, the model is generalizing properly from the training data we have provided it with.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOcycf-l2dGW"
      },
      "source": [
        "# Example 2 - non-linear function of 2 arguments\n",
        "\n",
        "Now that we have the first working neural network model, we can move to something a little more realistic. A non-linear function of two arguments. Let's try with:\n",
        "\n",
        "Y = x2 ** 2 * 5 - x1 * 3 + 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "-KDQS9v22Xtm",
        "outputId": "67959e78-2e2d-4511-9bd1-3f78991b4b23"
      },
      "outputs": [],
      "source": [
        "Xs1 = np.linspace(-10,10,50)\n",
        "Xs2 = np.linspace(-10,10,50)\n",
        "\n",
        "Xs1,Xs2 = np.meshgrid(Xs1,Xs2) # generate all possible pairs of x1 and x2\n",
        "\n",
        "Ys = (Xs2**2*5)-(Xs1*3) + 15\n",
        "\n",
        "# Plot the surface\n",
        "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
        "ax.plot_surface(Xs1, Xs2, Ys, vmin=Ys.min() * 2)\n",
        "\n",
        "#preparing torch tensors for learning\n",
        "\n",
        "inputs = torch.tensor(np.concatenate((Xs1.reshape((2500,1)),Xs2.reshape((2500,1))),axis=1), dtype=torch.float32)\n",
        "targets = torch.tensor(Ys.reshape((2500,1)), dtype=torch.float32)\n",
        "\n",
        "inputs.shape\n",
        "#targets.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTSUh4L93Gw_"
      },
      "source": [
        "This function is non-linear and has multiple arguments, let us create a model with non-linear activation function (We'll use ReLU for that), two hidden layers instead of one and we will use Adam optimizer to speed up the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIJeC7oL28Yx"
      },
      "outputs": [],
      "source": [
        "#model definition\n",
        "N_input=2\n",
        "N_hidden=25\n",
        "N_output=1\n",
        "\n",
        "#we will now have more hidden layers and more neurons in each of them\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(N_input, N_hidden),\n",
        "    nn.ReLU(),                       # we need non-linear activation functions\n",
        "    nn.Linear(N_hidden, N_hidden),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(N_hidden, N_hidden),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(N_hidden, N_output),\n",
        "    nn.ReLU()\n",
        ")\n",
        "\n",
        "criterion =  torch.nn.MSELoss()  # Mean Square Error\n",
        "\n",
        "# Construct the optimizer (Now we use Adam, a bit smarter optimizer)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # lr is the learning rate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38o02qTs3fdL"
      },
      "source": [
        "Again, a very similar training loop, with just a bit more iterations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSp_QruF3e36",
        "outputId": "5f9f4cef-41fc-4c27-83bc-9ed90a493c35"
      },
      "outputs": [],
      "source": [
        "\n",
        "N_epochs=50000\n",
        "loss_vals=[]\n",
        "\n",
        "# Gradient Descent\n",
        "for epoch in range(N_epochs):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(inputs)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, targets)\n",
        "    if epoch % 2000 == 0:                              #we will print fewer loss values\n",
        "        print('epoch: ', epoch, ' loss: ', loss.item())\n",
        "        loss_vals.append(loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # perform a backward pass (backpropagation)\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the parameters\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbckZiGf36ob"
      },
      "source": [
        "The optimizer learns very fast the first few steps and then slows down. Again we can plot the loss function improvement over *time*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "VpeBZ5zw3iiQ",
        "outputId": "de3cc353-56f3-48ec-e8ae-610e107bc2c8"
      },
      "outputs": [],
      "source": [
        "    plt.plot(range(len(loss_vals)),loss_vals,label=\"loss over epochs\")\n",
        "    plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EyNPUOg4Fnx"
      },
      "source": [
        "Again, we need to test if the function we have learned is generalizing beyond the training range:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "0ca9jwtl4Cr7",
        "outputId": "335a5a63-89c0-4b04-8fe4-8099eed4d839"
      },
      "outputs": [],
      "source": [
        "test_Xs1 = np.linspace(-20,20,15)\n",
        "test_Xs2 = np.linspace(-20,20,15)\n",
        "\n",
        "test_Xs1,test_Xs2 = np.meshgrid(test_Xs1,test_Xs2)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_data = torch.tensor(np.concatenate((test_Xs1.reshape((225,1)),test_Xs2.reshape((225,1))),axis=1), dtype=torch.float32)\n",
        "    test_output = model(test_data)\n",
        "\n",
        "test_mesh_output=test_output.reshape((15,15))\n",
        "\n",
        "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
        "ax.plot_surface(Xs1, Xs2, Ys,label=\"training data\")\n",
        "ax.scatter(test_Xs1,test_Xs2,test_mesh_output,label=\"testing data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARRKIneU4QJV"
      },
      "source": [
        "We have now seen two examples, where it was relatively easy for pytorch to fit the training data and generalize beyond it in the expected manner. Let us now see some situations, where this might not be so easy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eGBULbW4XeJ"
      },
      "source": [
        "# Example 3 - XOR-like function\n",
        "\n",
        "\n",
        "Let us first see how the underfitting looks like, based on the famous example of a non linear function similar to XOR. Let us consider a python function\n",
        "\n",
        "\n",
        "```\n",
        "def XOR_like(x1,x2):\n",
        "    if (x1>=1 and x2<=1) or (x2>=1 and x1<=1):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "```\n",
        "\n",
        "Can we train a single perceptron to learn it? This time, we will use some random data points instead of the regular mesh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "WDVem9104LdE",
        "outputId": "385cc79f-0bb6-4280-b17c-5fbae875a5ca"
      },
      "outputs": [],
      "source": [
        "# let us start with some random X1,X2 point pairs from 0,2 interval\n",
        "N_samples=100\n",
        "train_X1s=np.random.rand(N_samples)*2\n",
        "train_X2s=np.random.rand(N_samples)*2\n",
        "\n",
        "def XOR_like(x1,x2):\n",
        "    if (x1>=1 and x2<=1) or (x2>=1 and x1<=1):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "train_Ys=np.array([XOR_like(x1,x2) for x1,x2 in zip(train_X1s,train_X2s)])\n",
        "\n",
        "#prepare input and output data\n",
        "inputs = torch.tensor(np.concatenate((train_X1s.reshape((N_samples,1)),train_X2s.reshape((N_samples,1))),axis=1), dtype=torch.float32)\n",
        "targets = torch.tensor(train_Ys.reshape((N_samples,1)), dtype=torch.float32)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
        "ax.scatter(train_X1s, train_X2s, train_Ys,label=\"training data\",vmin=-1,c=train_Ys) #data is colored by the Y value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aSZpUwv5Cpm"
      },
      "source": [
        "As we have heard in the lecture, this function should be difficult for a simple 1-layer perceptron model. Let us test if this indeed is true. First we need to define the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgbZ6Z5r4op8"
      },
      "outputs": [],
      "source": [
        "\n",
        "#model definition\n",
        "N_input=2\n",
        "N_output=1\n",
        "\n",
        "#This is the simplest network so far, just a simple perceptron\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(N_input, N_output),\n",
        "    nn.ReLU(),                       # we need non-linear activation functions\n",
        "    )\n",
        "\n",
        "criterion =  torch.nn.MSELoss()  # Mean Square Error\n",
        "\n",
        "# Construct the optimizer (Now we use Adam, a bit smarter optimizer)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # lr is the learning rate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9189K2tv5Ubw"
      },
      "source": [
        "Once we have the model, we can try and train it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCZ5bXmH5PtX",
        "outputId": "8e44319b-ffaf-4cdd-b73c-e1e381db1cf5"
      },
      "outputs": [],
      "source": [
        "N_epochs=5000\n",
        "loss_vals=[]\n",
        "\n",
        "# Gradient Descent\n",
        "for epoch in range(N_epochs):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(inputs)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, targets)\n",
        "    if epoch % 500 == 0:                              #we will print fewer loss values\n",
        "        print('epoch: ', epoch, ' loss: ', loss.item())\n",
        "        loss_vals.append(loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # perform a backward pass (backpropagation)\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the parameters\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig0tPaqz5cmB"
      },
      "source": [
        "The loss function gets lower, but never quite reaches 0. let us see if this is indeed a problem or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "x6LHUR1-5aZ2",
        "outputId": "ba327e65-4a2b-4fd9-8939-fa4423e2deef"
      },
      "outputs": [],
      "source": [
        "N_samples=100\n",
        "test_X1s=np.random.rand(N_samples)*2\n",
        "test_X2s=np.random.rand(N_samples)*2\n",
        "\n",
        "test_Ys=np.array([XOR_like(x1,x2) for x1,x2 in zip(test_X1s,test_X2s)])\n",
        "\n",
        "#prepare test input  data\n",
        "test_inputs = torch.tensor(np.concatenate((test_X1s.reshape((N_samples,1)),test_X2s.reshape((N_samples,1))),axis=1), dtype=torch.float32)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_output = model(test_inputs)\n",
        "\n",
        "\n",
        "# fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
        "# ax.scatter(train_X1s, train_X2s, train_Ys,label=\"training data\",c=train_Ys,marker=\".\")\n",
        "# ax.scatter(test_X1s,test_X2s,test_output,label=\"testing data\",c=test_output,marker=\"o\")\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(train_X1s, train_X2s, label=\"training data\",c=train_Ys, s=4)\n",
        "ax.scatter(test_X1s,test_X2s,label=\"testing data\",c=test_output, s=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnoI7W295-7h"
      },
      "source": [
        "We can see that the model is definitely not a good fit to the data.\n",
        "\n",
        "Can we do better?\n",
        "**Exercise 1**\n",
        "\n",
        "Can you fill in the missing parts of the code so that we have a two-layer neural network that is able to fit the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "WTKs97Tv5mBX",
        "outputId": "a91dc207-1dec-4159-99f7-6c870dd88eff"
      },
      "outputs": [],
      "source": [
        "\n",
        "#model definition\n",
        "N_input=2\n",
        "N_hidden=20\n",
        "N_output=1\n",
        "\n",
        "#This is the simplest network so far, just a simple perceptron\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(N_input, N_hidden),\n",
        "    nn.ReLU(),                       # we need non-linear activation functions\n",
        "    nn.Linear(N_hidden, N_hidden),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(N_hidden, N_hidden),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(N_hidden, N_output),\n",
        "    nn.ReLU()\n",
        "    )\n",
        "\n",
        "criterion =  torch.nn.MSELoss()  # Mean Square Error\n",
        "\n",
        "# Construct the optimizer (Now we use Adam, a bit smarter optimizer)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # lr is the learning rate\n",
        "\n",
        "N_epochs=15000\n",
        "loss_vals=[]\n",
        "\n",
        "# Gradient Descent\n",
        "for epoch in range(N_epochs):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(inputs)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, targets)\n",
        "    if epoch % 500 == 0:                              #we will print fewer loss values\n",
        "        print('epoch: ', epoch, ' loss: ', loss.item())\n",
        "        loss_vals.append(loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # perform a backward pass (backpropagation)\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the parameters\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "# fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
        "# ax.scatter(train_X1s, train_X2s, train_Ys,label=\"training data\",c=train_Ys,marker=\".\")\n",
        "# ax.scatter(test_X1s,test_X2s,test_output,label=\"testing data\",c=test_output,marker=\"o\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "N_samples=100\n",
        "test_X1s=np.random.rand(N_samples)*2\n",
        "test_X2s=np.random.rand(N_samples)*2\n",
        "\n",
        "test_Ys=np.array([XOR_like(x1,x2) for x1,x2 in zip(test_X1s,test_X2s)])\n",
        "\n",
        "#prepare test input  data\n",
        "test_inputs = torch.tensor(np.concatenate((test_X1s.reshape((N_samples,1)),test_X2s.reshape((N_samples,1))),axis=1), dtype=torch.float32)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_output = model(test_inputs)\n",
        "\n",
        "\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.scatter(train_X1s, train_X2s, label=\"training data\",c=train_Ys, s=10)\n",
        "# ax.scatter(test_X1s,test_X2s,label=\"testing data\",c=test_output, s=10)\n",
        "\n",
        "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
        "ax.scatter(train_X1s, train_X2s, train_Ys,label=\"training data\",c=train_Ys,marker=\".\")\n",
        "ax.scatter(test_X1s,test_X2s,test_output,label=\"testing data\",c=test_output,marker=\"o\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEzIHUVT7bQ1"
      },
      "source": [
        "# Example 4 - Overfitting by undersampling\n",
        "\n",
        "Let us consider another common issue - overfitting caused by the training set not being representative of the larger testing set.\n",
        "\n",
        "Let us consider A cosine function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "Y7fs2ocj7t8h",
        "outputId": "9ac61434-b466-4ed5-a869-beb8083c48db"
      },
      "outputs": [],
      "source": [
        "cos_Xs=np.linspace(0,10,100)\n",
        "cos_Ys=np.cos(cos_Xs)\n",
        "plt.plot(cos_Xs,cos_Ys)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r28tpq8b70rK"
      },
      "source": [
        "If we prepare the training data from an interval that is too small, e.g. <0,3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtfYOLfz7xk7",
        "outputId": "8636b0d9-3938-4f46-bf5d-522652e8d36b"
      },
      "outputs": [],
      "source": [
        "train_cos_Xs=np.random.rand(100)*3\n",
        "train_cos_Ys=np.cos(train_cos_Xs)\n",
        "\n",
        "\n",
        "#we set the parameters of our simple network architecture\n",
        "N_input=1\n",
        "N_hidden=15\n",
        "N_output=1\n",
        "\n",
        "#create the model based on these parameters\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(N_input, N_hidden),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(N_hidden, N_output),\n",
        ")\n",
        "\n",
        "#convert our numpy arrays Xs and Ys into torch vectors we can use in training\n",
        "\n",
        "inputs = torch.tensor(train_cos_Xs.reshape((100,1)), dtype=torch.float32)\n",
        "targets = torch.tensor(train_cos_Ys.reshape((100,1)), dtype=torch.float32)\n",
        "\n",
        "criterion =  torch.nn.MSELoss()  # Mean Square Error\n",
        "\n",
        "# Construct the optimizer (Now we use Adam, a bit smarter optimizer)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # lr is the learning rate\n",
        "\n",
        "N_epochs=5000\n",
        "loss_vals=[]\n",
        "\n",
        "# Gradient Descent\n",
        "for epoch in range(N_epochs):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(inputs)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, targets)\n",
        "    if epoch % 500 == 0:                              #we will print fewer loss values\n",
        "        print('epoch: ', epoch, ' loss: ', loss.item())\n",
        "        loss_vals.append(loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # perform a backward pass (backpropagation)\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the parameters\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v09BM3IR78o_"
      },
      "source": [
        "The training seems to go well, but the function we have learned is notexactly a cosine function..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "-mPtjEmN749r",
        "outputId": "dd49177e-1a15-47fd-cab6-42caaeeafa75"
      },
      "outputs": [],
      "source": [
        "val_cos_Xs=np.linspace(0,6,100)\n",
        "val_cos_Ys=np.cos(val_cos_Xs)\n",
        "plt.plot(val_cos_Xs,val_cos_Ys)\n",
        "val_inputs=torch.tensor(val_cos_Xs.reshape((100,1)), dtype=torch.float32)\n",
        "with torch.no_grad():\n",
        "    val_output = model(val_inputs)\n",
        "plt.plot(val_cos_Xs,val_output,\"r:\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIWMigyo8w5Y"
      },
      "source": [
        "Indeed, we have fitted the part of the cosine function from 0 to 3 fairly well, but then the cosine function behaves differently...\n",
        "\n",
        "This is why we want to use a validation dataset to aid us in seeing the overfitting when it happens. Let us combine the two pieces of code together:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "KgLmOFFd8AzP",
        "outputId": "18cadc88-2843-49bb-e37b-508f8b05b851"
      },
      "outputs": [],
      "source": [
        "# we create the training data\n",
        "train_cos_Xs=np.random.rand(200)*3\n",
        "train_cos_Ys=np.cos(train_cos_Xs)\n",
        "\n",
        "# and the validation data\n",
        "val_cos_Xs=np.random.rand(30)*6\n",
        "val_cos_Ys=np.cos(val_cos_Xs)\n",
        "\n",
        "\n",
        "#convert our numpy arrays Xs and Ys into torch vectors we can use in training\n",
        "\n",
        "inputs = torch.tensor(train_cos_Xs.reshape((200,1)), dtype=torch.float32)\n",
        "targets = torch.tensor(train_cos_Ys.reshape((200,1)), dtype=torch.float32)\n",
        "val_inputs=torch.tensor(val_cos_Xs.reshape((30,1)), dtype=torch.float32)\n",
        "val_targets=torch.tensor(val_cos_Ys.reshape((30,1)), dtype=torch.float32)\n",
        "\n",
        "#we set the parameters of our simple network architecture\n",
        "N_input=1\n",
        "N_hidden=25\n",
        "N_output=1\n",
        "\n",
        "#create the model based on these parameters\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(N_input, N_hidden),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(N_hidden, N_output),\n",
        ")\n",
        "\n",
        "criterion =  torch.nn.MSELoss()  # Mean Square Error\n",
        "\n",
        "# Construct the optimizer (Now we use Adam, a bit smarter optimizer)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # lr is the learning rate\n",
        "\n",
        "N_epochs=5000\n",
        "loss_vals=[]\n",
        "valid_vals=[]\n",
        "\n",
        "# Gradient Descent\n",
        "for epoch in range(N_epochs):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(inputs)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, targets)\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # perform a backward pass (backpropagation)\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0:                              #we will print fewer loss values\n",
        "        loss_vals.append(loss.item())\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_output = model(val_inputs)\n",
        "            val_loss=criterion(val_output,val_targets)\n",
        "            valid_vals.append(val_loss.item())\n",
        "\n",
        "        print('epoch: ', epoch, ' loss: ', loss.item(),\"validation loss: \",val_loss.item())\n",
        "\n",
        "#let us see the behavior of both training and validation loss\n",
        "plt.plot(loss_vals,label=\"training loss\")\n",
        "plt.plot(valid_vals,label=\"validation loss\")\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvOskzQu9mST"
      },
      "source": [
        "Clearly, while the training error seems to go down with training, but it is not the case for the validation error indicating that we have a severe overfitting issue.\n",
        "\n",
        "**Exercise 2**\n",
        "\n",
        "Can you modify the above code snippet, so that the model is fitting the data properly? Hint: we need to make sure that the training data is sampled from the range similar to the validation data..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGWuskOk9Ktj"
      },
      "outputs": [],
      "source": [
        "# we create the training data\n",
        "train_cos_Xs=np.random.rand(200)*6\n",
        "train_cos_Ys=np.cos(train_cos_Xs)\n",
        "\n",
        "# and the validation data\n",
        "val_cos_Xs=np.random.rand(30)*6\n",
        "val_cos_Ys=np.cos(val_cos_Xs)\n",
        "\n",
        "\n",
        "#convert our numpy arrays Xs and Ys into torch vectors we can use in training\n",
        "\n",
        "inputs = torch.tensor(train_cos_Xs.reshape((200,1)), dtype=torch.float32)\n",
        "targets = torch.tensor(train_cos_Ys.reshape((200,1)), dtype=torch.float32)\n",
        "val_inputs=torch.tensor(val_cos_Xs.reshape((30,1)), dtype=torch.float32)\n",
        "val_targets=torch.tensor(val_cos_Ys.reshape((30,1)), dtype=torch.float32)\n",
        "\n",
        "#we set the parameters of our simple network architecture\n",
        "N_input=1\n",
        "N_hidden=25\n",
        "N_output=1\n",
        "\n",
        "#create the model based on these parameters\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(N_input, N_hidden),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(N_hidden, N_output),\n",
        ")\n",
        "\n",
        "criterion =  torch.nn.MSELoss()  # Mean Square Error\n",
        "\n",
        "# Construct the optimizer (Now we use Adam, a bit smarter optimizer)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # lr is the learning rate\n",
        "\n",
        "N_epochs=5000\n",
        "loss_vals=[]\n",
        "valid_vals=[]\n",
        "\n",
        "# Gradient Descent\n",
        "for epoch in range(N_epochs):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(inputs)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, targets)\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # perform a backward pass (backpropagation)\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0:                              #we will print fewer loss values\n",
        "        loss_vals.append(loss.item())\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_output = model(val_inputs)\n",
        "            val_loss=criterion(val_output,val_targets)\n",
        "            valid_vals.append(val_loss.item())\n",
        "\n",
        "        print('epoch: ', epoch, ' loss: ', loss.item(),\"validation loss: \",val_loss.item())\n",
        "\n",
        "#let us see the behavior of both training and validation loss\n",
        "plt.plot(loss_vals,label=\"training loss\")\n",
        "plt.plot(valid_vals,label=\"validation loss\")\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDrKYozI_IK7"
      },
      "source": [
        "# Example 5 - overfitting noise with large capacity network\n",
        "\n",
        "Let us consider a relatively simple function: y=x**2-5, but now add a little noise to the samples, and let us assume that the data is expensive to generate, so we will have relatively few datapoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "e8pIISZ9_UJp",
        "outputId": "072437db-3db6-4872-d96c-0553fd6991a5"
      },
      "outputs": [],
      "source": [
        "# we create the training data\n",
        "train_noisy_Xs=np.random.rand(20)*10\n",
        "train_noisy_Xs.sort()\n",
        "train_noisy_Ys=train_noisy_Xs**2-5+np.random.rand(20)*40-20 #20 is the average of the noise\n",
        "\n",
        "\n",
        "# and the validation data\n",
        "val_noisy_Xs=np.random.rand(20)*10\n",
        "val_noisy_Xs.sort()\n",
        "val_noisy_Ys=val_noisy_Xs**2-5+np.random.rand(20)*40-20\n",
        "\n",
        "#plot what is generated\n",
        "\n",
        "plt.scatter(train_noisy_Xs,train_noisy_Ys,label=\"training data\")\n",
        "plt.scatter(val_noisy_Xs,val_noisy_Ys,label=\"validation data\")\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "#convert our numpy arrays Xs and Ys into torch vectors we can use in training\n",
        "\n",
        "inputs = torch.tensor(train_noisy_Xs.reshape((20,1)), dtype=torch.float32)\n",
        "targets = torch.tensor(train_noisy_Ys.reshape((20,1)), dtype=torch.float32)\n",
        "\n",
        "val_inputs=torch.tensor(val_noisy_Xs.reshape((20,1)), dtype=torch.float32)\n",
        "val_targets=torch.tensor(val_noisy_Ys.reshape((20,1)), dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr89h4J6_ZJ2"
      },
      "source": [
        "Now, since we have so few datapoints, we feel that we need to compensate for what we lack in the data department by having a more complex computational model. Let us for a moment accept that this might be a good idea..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAHdKt_K_U9s",
        "outputId": "dd013c15-e2ca-4454-944a-c627f579060b"
      },
      "outputs": [],
      "source": [
        "\n",
        "#we set the parameters of our relatively complex network architecture\n",
        "N_input=1\n",
        "N_hidden=500\n",
        "N_output=1\n",
        "\n",
        "#create the model based on these parameters\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(N_input, N_hidden),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(N_hidden, N_hidden),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(N_hidden, N_output),\n",
        ")\n",
        "\n",
        "criterion =  torch.nn.MSELoss()  # Mean Square Error\n",
        "\n",
        "# Construct the optimizer (Now we use Adam, a bit smarter optimizer)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # lr is the learning rate\n",
        "\n",
        "N_epochs=10000\n",
        "loss_vals=[]\n",
        "valid_vals=[]\n",
        "\n",
        "# Gradient Descent\n",
        "for epoch in range(N_epochs):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(inputs)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, targets)\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # perform a backward pass (backpropagation)\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 1000 == 0:                              #we will print fewer loss values\n",
        "        loss_vals.append(loss.item())\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_output = model(val_inputs)\n",
        "            val_loss=criterion(val_output,val_targets)\n",
        "            valid_vals.append(val_loss.item())\n",
        "\n",
        "        print('epoch: ', epoch, ' loss: ', loss.item(),\"validation loss: \",val_loss.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsKwv1doA9HE"
      },
      "source": [
        "Clearly, there is some overfitting going on, but is it a different kind of overfitting? Let us do some plotting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "7H5q5J_KA3iG",
        "outputId": "cd5b7b16-6a10-484d-b63b-62dd2f14d3d6"
      },
      "outputs": [],
      "source": [
        "plt.plot(loss_vals,label=\"training loss\")\n",
        "plt.plot(valid_vals,label=\"validation loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(train_noisy_Xs,train_noisy_Ys,label=\"training data\")\n",
        "plt.plot(val_noisy_Xs,val_noisy_Ys,\"r-\",label=\"validation data\")\n",
        "plt.plot(inputs,y_pred.detach().numpy(),\"bx\",label=\"training output\")\n",
        "plt.plot(val_inputs,val_output.detach().numpy(),\"rx\",label=\"validation output\")\n",
        "actual_Xs=np.linspace(0,10,100)\n",
        "plt.plot(actual_Xs,actual_Xs**2-5,\"g:\",label=\"actual function\")\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY4so-zoBHCc"
      },
      "source": [
        "Indeed, we have fitted the training data fairly well, but the presence of noise in the data (both training and validation), we can see that the model fits the blue curve better than we want it to (closer to the blue curve than to the green dots). It can be argued that this is indeed undersampling as well as in the previous example - we don't have enough data to generalize well.\n",
        "\n",
        "In order tro fix that, we should have a relatively simpler model and train it on much more data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "DWOa8HOL_ukB",
        "outputId": "96b469ea-8a4c-4a9a-c905-966c88f34782"
      },
      "outputs": [],
      "source": [
        "#let us createlarger training data\n",
        "train_noisy_Xs=np.random.rand(1000)*10\n",
        "train_noisy_Xs.sort()\n",
        "train_noisy_Ys=train_noisy_Xs**2-5+np.random.rand(1000)*40-20 #20 is the average of the noise\n",
        "\n",
        "\n",
        "# and the validation data\n",
        "val_noisy_Xs=np.random.rand(200)*10\n",
        "val_noisy_Xs.sort()\n",
        "val_noisy_Ys=val_noisy_Xs**2-5+np.random.rand(200)*40-20\n",
        "\n",
        "#plot what is generated\n",
        "\n",
        "plt.plot(train_noisy_Xs,train_noisy_Ys,\"r:\",label=\"training data\")\n",
        "plt.plot(val_noisy_Xs,val_noisy_Ys,\"g-\",label=\"validation data\")\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "#convert our numpy arrays Xs and Ys into torch vectors we can use in training\n",
        "\n",
        "inputs = torch.tensor(train_noisy_Xs.reshape((1000,1)), dtype=torch.float32)\n",
        "targets = torch.tensor(train_noisy_Ys.reshape((1000,1)), dtype=torch.float32)\n",
        "\n",
        "val_inputs=torch.tensor(val_noisy_Xs.reshape((200,1)), dtype=torch.float32)\n",
        "val_targets=torch.tensor(val_noisy_Ys.reshape((200,1)), dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVPzpbzsB0HS"
      },
      "source": [
        "**Exercise 3** can you now use this data to train a simpler network (just one hidden layer with 50 neurons on this data? Does it solve the problem?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#we set the parameters of our relatively complex network architecture\n",
        "N_input=1\n",
        "N_hidden=50\n",
        "N_output=1\n",
        "\n",
        "#create the model based on these parameters\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(N_input, N_hidden),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(N_hidden, N_hidden),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(N_hidden, N_output),\n",
        ")\n",
        "\n",
        "criterion =  torch.nn.MSELoss()  # Mean Square Error\n",
        "\n",
        "# Construct the optimizer (Now we use Adam, a bit smarter optimizer)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # lr is the learning rate\n",
        "\n",
        "N_epochs=10000\n",
        "loss_vals=[]\n",
        "valid_vals=[]\n",
        "\n",
        "# Gradient Descent\n",
        "for epoch in range(N_epochs):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(inputs)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, targets)\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # perform a backward pass (backpropagation)\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 1000 == 0:                              #we will print fewer loss values\n",
        "        loss_vals.append(loss.item())\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_output = model(val_inputs)\n",
        "            val_loss=criterion(val_output,val_targets)\n",
        "            valid_vals.append(val_loss.item())\n",
        "\n",
        "        print('epoch: ', epoch, ' loss: ', loss.item(),\"validation loss: \",val_loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(loss_vals,label=\"training loss\")\n",
        "plt.plot(valid_vals,label=\"validation loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(train_noisy_Xs,train_noisy_Ys,label=\"training data\")\n",
        "plt.plot(val_noisy_Xs,val_noisy_Ys,\"r-\",label=\"validation data\")\n",
        "plt.plot(inputs,y_pred.detach().numpy(),\"bx\",label=\"training output\")\n",
        "plt.plot(val_inputs,val_output.detach().numpy(),\"rx\",label=\"validation output\")\n",
        "actual_Xs=np.linspace(0,10,100)\n",
        "plt.plot(actual_Xs,actual_Xs**2-5,\"g:\",label=\"actual function\")\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPLNjreBCWLk"
      },
      "source": [
        "Even though the Loss value remains non-zero throughout the training, this is expected, as there is now noise in the training and validation data. Nonetheless, the fit to the data is relatively good.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rma0fPUhpulA"
      },
      "source": [
        "# Assignments for homework\n",
        "\n",
        "As different groups at different Universities will use different scoring criteria, this might be completely optional for some of you and compulsory for some of you. It might or might not contribute to your grade. **Please consult with your instructors**\n",
        "\n",
        "Homework assignments:\n",
        "\n",
        "1.   Please take a look at the [PyTorch beginner tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). We did not use the dataset and dataloader API, but this should not stop you. Choose one of the examples from today's lecture and extend it so that it uses datasets and dataloaders\n",
        "2.   One more thing that you might want to consider is re-writing the training loops to use the dataloaders and minibatches as in the [pytorch tutorial](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHWcDXIgCO1V"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
