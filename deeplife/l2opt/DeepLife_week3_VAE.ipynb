{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Week 3: Autoencoders and Variational autoencoders\n","\n","### Carl Herrmann"]},{"cell_type":"markdown","metadata":{"id":"atPphSak2S0l"},"source":["## 1 - Motivation\n","\n","Imagine that we have a large, high-dimensional dataset. For example, imagine we have a dataset consisting of thousands of images. Each image  is made up of hundreds of pixels, so each data point has hundreds of dimensions. The **[manifold hypothesis](https://deepai.org/machine-learning-glossary-and-terms/manifold-hypothesis)** states that real-world high-dimensional data actually consists of low-dimensional data that is embedded in the high-dimensional space. This means that, while the actual data itself might have hundreds of dimensions, the underlying structure of the data can be sufficiently described using only a few dimensions.\n","\n","This is the motivation behind dimensionality reduction techniques, which try to take high-dimensional data and project it onto a lower-dimensional surface. For humans who visualize most things in 2D (or sometimes 3D), this usually means projecting the data onto a 2D surface. Examples of dimensionality reduction techniques include [principal component analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) and [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding). "]},{"cell_type":"markdown","metadata":{"id":"EOt_pQVB2S0l"},"source":["Neural networks are often used in the **supervised learning** context, where data consists of pairs $(x, y)$ and the network learns a function $f:X \\to Y$. This context applies to both regression (where $y$ is a continuous function of $x$) and classification (where $y$ is a discrete label for $x$). However, neural networks have shown considerable power in the **unsupervised learning** context, where data just consists of points $x$. There are no \"targets\" or \"labels\" $y$. Instead, the goal is to learn and understand the structure of the data. In the case of dimensionality reduction, the goal is to find a low-dimensional representation of the data."]},{"cell_type":"markdown","metadata":{"id":"vnt5sKiADS7R"},"source":["## 2 - Principal component analysis"]},{"cell_type":"markdown","metadata":{"id":"_guRDsusDO-R"},"source":["One simple linear dimensional reduction technique is Principal Component Analysis (PCA). Let us start with a simple PCA analysis on the MNIST data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z3G5oponDYCz"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","from matplotlib import cm\n","import torch; torch.manual_seed(0)\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils\n","import torch.distributions\n","import numpy as np\n","import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n","import torchvision\n","from torchvision import datasets, transforms\n"]},{"cell_type":"markdown","metadata":{"id":"U6dITjIuGmj7"},"source":["Now load the MNIST images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1032,"status":"ok","timestamp":1708114038144,"user":{"displayName":"Carl Herrmann","userId":"05516243901234844189"},"user_tz":-60},"id":"r_QKVXCNFwXZ","outputId":"12e5f433-663f-4d81-9f8e-9ec5593dcb07"},"outputs":[],"source":["batch_size = 32\n","\n","train_dataset = datasets.MNIST('./data',\n","                               train=True,\n","                               download=True,\n","                               transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                   ]))\n","\n","test_dataset = datasets.MNIST('./data',\n","                                    train=False,\n","                               transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                   ]))\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                                batch_size=batch_size,\n","                                                shuffle=False)\n","\n","train_images = train_loader.dataset.data.detach().numpy().reshape(-1, 28*28)\n","print(f'Num training images: {len(train_images)},\\tmin val: {train_images.min():.3f},\\tmax val: {train_images.max():.3f}')\n"]},{"cell_type":"markdown","metadata":{},"source":["We learn the first principal components on the training dataset (=60000 digit images) and plot the principal components"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"elapsed":4763,"status":"ok","timestamp":1708114179267,"user":{"displayName":"Carl Herrmann","userId":"05516243901234844189"},"user_tz":-60},"id":"HorCTulyJNXh","outputId":"1153386d-5d27-45fd-f600-bb42ad877c37"},"outputs":[],"source":["n_components = 10\n","pca = PCA(n_components=n_components)\n","pca.fit(train_images)\n","fig = plt.figure(figsize=(n_components*np.power(n_components, 1/5),3))\n","axes = []\n","for i in range(n_components):\n","    ax_ = fig.add_subplot(1, n_components, i+1)\n","    ax_.imshow(pca.components_[i].reshape(28,28), interpolation='nearest', clim=(-.15, .15));\n","    ax_.set_title(f'PC {i+1}')\n","    ax_.axis('off')\n","\n","    axes.append(ax_)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJWjE8jWhOkB"},"outputs":[],"source":["n_pc = [1,2,3,5,10,15,20,30,50,100,200]\n","\n","# Try PCA on first ten test images\n","n_images = 9\n","n_components = len(n_pc)\n","test_images = np.array(test_loader.dataset.data[:n_images].numpy().reshape((n_images,784)))\n","\n","# Plot the first ten test images and the corresponding outputs\n","fig = plt.figure(figsize=(10, n_components))\n","ax = fig.add_subplot(111)\n","images_in = [test_images[j].reshape(-1, 28) for j in range(n_images)]\n","image = np.concatenate(images_in, axis=1)\n","\n","for i in n_pc:\n","    pca = PCA(n_components=i)\n","    pca.fit(train_images)\n","    test_outputs = pca.inverse_transform(pca.transform(test_images))\n","    images_out = [test_outputs[j].reshape(-1, 28) for j in range(n_images)]\n","    image_out = np.concatenate(images_out, axis=1)\n","    image = np.concatenate([image, image_out])\n","\n","ax.matshow(image)\n","ax.set_xticks(np.array([]))\n","y_lim = ax.get_ylim()\n","y_scale = (y_lim[0] - y_lim[1])/(n_components+1)\n","ax.set_yticks(np.linspace(.5, n_components+.5, n_components+1) * y_scale)\n","\n","a = [0]\n","a.extend(n_pc)\n","ax.set_yticklabels(a)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["## 3 - Autoencoders\n","\n","Autoencoders are a special kind of neural network used to perform dimensionality reduction. We can think of autoencoders as being composed of two networks, an **encoder** $f$ and a **decoder** $h$.\n","\n","The encoder learns a non-linear transformation $f:X \\to Z$ that projects the data from the original high-dimensional input space $X$ to a lower-dimensional **latent space** $Z$. We call $z = f(x)$ a **latent vector**. A latent vector is a low-dimensional representation of a data point that contains information about $x$. The transformation $f$ should have certain properties, like similar values of $x$ should have similar latent vectors (and dissimilar values of $x$ should have dissimilar latent vectors).\n","\n","A decoder learns a non-linear transformation $h: Z \\to X$ that projects the latent vectors back into the original high-dimensional input space $X$. This transformation should take the latent vector $z = f(x)$ and reconstruct the original input data $\\hat{x} = h(z) = h(f(x))$.\n","\n","An autoencoder is just the composition of the encoder and the decoder $h(f(x))$. The autoencoder is trained to minimize the difference between the input $x$ and the reconstruction $\\hat{x}$ using a kind of **reconstruction loss**. Because the autoencoder is trained as a whole (we say it's trained \"end-to-end\"), we simultaneosly optimize the encoder and the decoder."]},{"cell_type":"markdown","metadata":{"id":"gpHocADR2S0m"},"source":["Below is an implementation of an autoencoder written in PyTorch. We apply it to the MNIST dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vdPWQ2zq2S0n"},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'mps'\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device\n"]},{"cell_type":"markdown","metadata":{"id":"0LUzLq7b2S0o"},"source":["Below we write the `Encoder` class by subclassing `torch.nn.Module`, which lets us define the `__init__` method storing layers as an attribute, and a `forward` method describing the forward pass of the network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THB9oQNr2S0o"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, latent_dims,input_dim):\n","        super(Encoder, self).__init__()\n","        self.linear1 = nn.Linear(input_dim**2, 512)\n","        self.linear2 = nn.Linear(512, latent_dims)\n","\n","    def forward(self, x):\n","        x = torch.flatten(x, start_dim=1)\n","        x = F.relu(self.linear1(x))\n","        return self.linear2(x)\n"]},{"cell_type":"markdown","metadata":{"id":"7uqokEdT2S0p"},"source":["We do something similar for the `Decoder` class, ensuring we reshape the output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iow-o15M2S0q"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, latent_dims,input_dim):\n","        super(Decoder, self).__init__()\n","        self.linear1 = nn.Linear(latent_dims, 512)\n","        self.linear2 = nn.Linear(512, input_dim**2)\n","\n","    def forward(self, z):\n","        z = F.relu(self.linear1(z))\n","        z = torch.sigmoid(self.linear2(z))\n","        return z.reshape((-1, 1, input_dim, input_dim))\n"]},{"cell_type":"markdown","metadata":{"id":"xRNU9p5E2S0q"},"source":["FInally, we write an `Autoencoder` class that combines these two. Note that we could have easily written this entire autoencoder as a single neural network, but splitting them in two makes it conceptually clearer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QIHvgMjH2S0q"},"outputs":[],"source":["class Autoencoder(nn.Module):\n","    def __init__(self, latent_dims,input_dim):\n","        super(Autoencoder, self).__init__()\n","        self.encoder = Encoder(latent_dims,input_dim)\n","        self.decoder = Decoder(latent_dims,input_dim)\n","\n","    def forward(self, x):\n","        z = self.encoder(x)\n","        return self.decoder(z)\n"]},{"cell_type":"markdown","metadata":{"id":"12MggT-h2S0q"},"source":["Next, we will write some code to train the autoencoder on the MNIST dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2GC8R2g32S0r"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","def train(autoencoder: Autoencoder, data: Dataset, epochs: int = 20):\n","    opt = torch.optim.Adam(autoencoder.parameters())\n","    losses = []\n","    for epoch in range(epochs):\n","        print(\"epoch: \",epoch)\n","        loss_e = 0\n","        for x, y in data:\n","            x = x.to(device) # GPU\n","            opt.zero_grad()\n","            x_hat = autoencoder(x)\n","            loss = ((x - x_hat)**2).sum()\n","            loss.backward()\n","            opt.step()\n","            loss_e += loss.to('cpu').detach().numpy()\n","        losses.append(loss_e/len(data))\n","    return autoencoder, losses\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":419414,"status":"ok","timestamp":1706626317829,"user":{"displayName":"Carl Herrmann","userId":"05516243901234844189"},"user_tz":-60},"id":"b6kE3XC72S0r","outputId":"d1e724f5-ba40-497e-f6a2-c393aedef636"},"outputs":[],"source":["latent_dims = 2\n","input_dim = 28\n","autoencoder = Autoencoder(latent_dims,input_dim).to(device) # GPU\n","\n","train_data = torch.utils.data.DataLoader(\n","        torchvision.datasets.MNIST('./data',\n","               transform=torchvision.transforms.ToTensor(),\n","               download=True),\n","        batch_size=128,\n","        shuffle=True)\n","\n","autoencoder, losses = train(autoencoder, train_data,epochs=20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":801},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1706626324908,"user":{"displayName":"Carl Herrmann","userId":"05516243901234844189"},"user_tz":-60},"id":"1Dq-TaB-vCeJ","outputId":"c8680709-39f8-4175-fcca-33b4df7a2617"},"outputs":[],"source":["plt.plot(losses)\n"]},{"cell_type":"markdown","metadata":{},"source":["> **Question 1**: Define a training and validation dataset and plot both loss curves. Compute the validation loss at the end of each epoch. You can reuse the dataloaders from the PCA section! Can you comment on the validation vs training loss? You can check [this website](https://towardsdatascience.com/what-your-validation-loss-is-lower-than-your-training-loss-this-is-why-5e92e0b1747e)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mnist = torchvision.datasets.MNIST('./data', transform=torchvision.transforms.ToTensor(), download=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import random_split\n","\n","def train_val(autoencoder: Autoencoder, train_data: DataLoader, validation_data: DataLoader, epochs: int = 20):\n","    opt = torch.optim.Adam(autoencoder.parameters())\n","    losses = []\n","    val_losses = []\n","\n","    for epoch in range(epochs):\n","        autoencoder.train()\n","\n","        print(\"epoch: \",epoch)\n","        loss_e = 0\n","        for x, y in train_data:\n","            x = x.to(device) # GPU\n","            opt.zero_grad()\n","            x_hat = autoencoder(x)\n","            loss = ((x - x_hat)**2).sum()\n","            loss.backward()\n","            opt.step()\n","            loss_e += loss.to('cpu').detach().numpy()# / x.shape[0]\n","        losses.append(loss_e/len(train_data.dataset))\n","        # losses.append(loss_e/len(train_data))\n","        # losses.append(loss_e)\n","\n","        autoencoder.eval()\n","\n","        val_loss_e = 0\n","\n","        with torch.no_grad():\n","            for x, y in validation_data:\n","                x = x.to(device)\n","                x_hat = autoencoder(x)\n","                val_loss = ((x - x_hat)**2).sum()\n","                val_loss_e += val_loss.to('cpu').detach().numpy()# / x.shape[0]\n","\n","        val_losses.append(val_loss_e / len(validation_data.dataset))\n","        # val_losses.append(val_loss_e / len(validation_data))\n","        # val_losses.append(val_loss_e)\n","\n","    return autoencoder, losses, val_losses\n","\n","train_dataset, validation_dataset, _ = random_split(mnist, [0.45, 0.05, 0.5])\n","\n","train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","validation_loader = DataLoader(validation_dataset, batch_size=128, shuffle=False)\n","\n","autoencoder = Autoencoder(latent_dims, input_dim).to(device)\n","autoencoder, losses, val_losses = train_val(autoencoder, train_loader, validation_loader, epochs=20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots()\n","\n","ax.plot(losses, label='Training')\n","ax.plot(val_losses, label='Validation')\n","ax.legend()\n"]},{"cell_type":"markdown","metadata":{},"source":["### SOLUTION"]},{"cell_type":"markdown","metadata":{},"source":["***"]},{"cell_type":"markdown","metadata":{},"source":["> **Question 2**: Test an overcomplete, linear AE, and plot training and test loss. Compare it to the losses of the undercomplete version."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["autoencoder = Autoencoder(800, input_dim).to(device)\n","autoencoder, losses_over, val_losses_over = train_val(autoencoder, train_loader, validation_loader, epochs=20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots()\n","\n","ax.plot(val_losses, label='Undercomplete')\n","ax.plot(val_losses_over, label='Overcomplete')\n","ax.legend()\n"]},{"cell_type":"markdown","metadata":{},"source":["### SOLUTION"]},{"cell_type":"markdown","metadata":{},"source":["***"]},{"cell_type":"markdown","metadata":{"id":"UBG1qxSB2S0r"},"source":[" What should we look at once we've trained an autoencoder?\n","\n","1. Look at the latent space. If the latent space is 2-dimensional, then we can transform a batch of inputs $x$ using the encoder and make a scatterplot of the output vectors. Since we also have access to labels for MNIST, we can colour code the outputs to see what they look like.\n","2. Sample the latent space to produce output. If the latent space is 2-dimensional, we can sample latent vectors $z$ from the latent space over a uniform grid and plot the decoded latent vectors on a grid."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xU0Szm6x2S0r"},"outputs":[],"source":["def plot_latent(autoencoder: Autoencoder, data: DataLoader, num_batches: int =100):\n","    for i, (x, y) in enumerate(data):\n","        z = autoencoder.encoder(x.to(device))\n","        z = z.to('cpu').detach().numpy()\n","        plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10',s=1)\n","        if i > num_batches:\n","            plt.colorbar()\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":854},"executionInfo":{"elapsed":5258,"status":"ok","timestamp":1706626342321,"user":{"displayName":"Carl Herrmann","userId":"05516243901234844189"},"user_tz":-60},"id":"OEmAcn9p2S0r","outputId":"a96f3811-8ae5-46ad-d442-32df17545dc9"},"outputs":[],"source":["plot_latent(autoencoder, train_loader,num_batches=100)\n"]},{"cell_type":"markdown","metadata":{"id":"B79jK_7O2S0s"},"source":["The resulting latent vectors cluster similar digits together. We can also sample uniformly from the latent space and see how the decoder reconstructs inputs from arbitrary latent vectors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ISRzsVBF2S0s"},"outputs":[],"source":["def plot_reconstructed(autoencoder: Autoencoder, r0=(-5, 30), r1=(-20, 15), n=12):\n","    w = 28\n","    img = np.zeros((n*w, n*w))\n","    for i, y in enumerate(np.linspace(*r1, n)):\n","        for j, x in enumerate(np.linspace(*r0, n)):\n","            z = torch.Tensor([[x, y]]).to(device)\n","            x_hat = autoencoder.decoder(z)\n","            x_hat = x_hat.reshape(28, 28).to('cpu').detach().numpy()\n","            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat\n","    plt.imshow(img, extent=[*r0, *r1])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":785},"executionInfo":{"elapsed":4226,"status":"ok","timestamp":1706626380805,"user":{"displayName":"Carl Herrmann","userId":"05516243901234844189"},"user_tz":-60},"id":"t25B-I6p2S0s","outputId":"bbc635f1-5dc1-4412-843c-927a6ff87115"},"outputs":[],"source":["plot_reconstructed(autoencoder)\n"]},{"cell_type":"markdown","metadata":{"id":"NGwCTCh-2S0t"},"source":["We intentionally plot the reconstructed latent vectors using approximately the same range of values taken on by the actual latent vectors. We can see that the reconstructed latent vectors look like digits, and the kind of digit corresponds to the location of the latent vector in the latent space."]},{"cell_type":"markdown","metadata":{"id":"H5MmyCLz2S0t"},"source":["You may have noticed that there are \"gaps\" in the latent space, where data is never mapped to. This becomes a problem when we try to use autoencoders as **generative models**. The goal of generative models is to take a data set $X$ and produce more data points from the same distribution that $X$ is drawn from. For autoencoders, this means sampling latent vectors $z \\sim Z$ and then decoding the latent vectors to produce images. If we sample a latent vector from a region in the latent space that was never seen by the decoder during training, the output might not make any sense at all. We see this in the top left corner of the `plot_reconstructed` output, which is empty in the latent space, and the corresponding decoded digit does not match any existing digits."]},{"cell_type":"markdown","metadata":{"id":"XMfKXG5C2S0t"},"source":["## 4 - Variational Autoencoders\n","\n","The only constraint on the latent vector representation for traditional autoencoders is that latent vectors should be easily decodable back into the original image. As a result, the latent space $Z$ can become disjoint and non-continuous. Variational autoencoders try to solve this problem.\n","\n","In traditional autoencoders, inputs are mapped deterministically to a latent vector $z = f(x)$. In variational autoencoders, inputs are mapped to a probability distribution over latent vectors, and a latent vector is then sampled from that distribution. The decoder becomes more robust at decoding latent vectors as a result.\n","\n","Specifically, instead of mapping the input $x$ to a latent vector $z = f(x)$, we map it instead to a mean vector $\\mu(x)$ and a vector of standard deviations $\\sigma(x)$. These parametrize a diagonal Gaussian distribution $\\mathcal{N}(\\mu, \\sigma)$, from which we then sample a latent vector $z \\sim \\mathcal{N}(\\mu, \\sigma)$.\n","\n","This is generally accomplished by replacing the last layer of a traditional autoencoder with two layers, each of which output $\\mu(x)$ and $\\sigma(x)$. An exponential activation is often added to $\\sigma(x)$ to ensure the result is positive."]},{"cell_type":"markdown","metadata":{"id":"mNWb6w7X2S0t"},"source":["However, this does not completely solve the problem. There may still be gaps in the latent space because the outputted means may be significantly different and the standard deviations may be small. To reduce that, we add an **auxillary loss** that penalizes the distribution $p(z \\mid x)$ for being too far from the standard normal distribution $\\mathcal{N}(0, 1)$. This penalty term is the KL divergence between $p(z \\mid x)$ and $\\mathcal{N}(0, 1)$, which is given by\n","$$\n","\\mathbb{KL}\\left( \\mathcal{N}(\\mu, \\sigma) \\parallel \\mathcal{N}(0, 1) \\right) = \\sum_{x \\in X} \\left( \\frac{1}{2}\\sigma^2 + \\frac{1}{2}\\mu^2 - \\log \\sigma - \\frac{1}{2} \\right)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"_VCwh_ih2S0u"},"source":["This expression applies to two univariate Gaussian distributions. Extending it to our diagonal Gaussian distributions is not difficult; we simply sum the KL divergence for each dimension.\n","\n","This loss is useful for two reasons. First, we cannot train the encoder network by gradient descent without it, since gradients cannot flow through sampling (which is a non-differentiable operation). Second, by penalizing the KL divergence in this manner, we can encourage the latent vectors to occupy a more centralized and uniform location. In essence, we force the encoder to find latent vectors that approximately follow a standard Gaussian distribution that the decoder can then effectively decode."]},{"cell_type":"markdown","metadata":{"id":"9D4vD_gu2S0u"},"source":["To implement this, we do not need to change the `Decoder` class. We only need to change the `Encoder` class to produce $\\mu(x)$ and $\\sigma(x)$, and then use these to sample a latent vector. We also use this class to keep track of the KL divergence loss term."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FtBSMERo2S0u"},"outputs":[],"source":["class VariationalEncoder(nn.Module):\n","    def __init__(self, latent_dims,input_dim):\n","        super(VariationalEncoder, self).__init__()\n","        self.linear1 = nn.Linear(input_dim**2, 512)\n","        self.linear2 = nn.Linear(512, latent_dims)\n","        self.linear3 = nn.Linear(512, latent_dims)\n","\n","        self.N = torch.distributions.Normal(0, 1)\n","        self.N.loc = self.N.loc.to(device)\n","        self.N.scale = self.N.scale.to(device)\n","        self.kl = 0\n","\n","    def forward(self, x):\n","        x = torch.flatten(x, start_dim=1)\n","        x = F.relu(self.linear1(x))\n","        sigma = torch.exp(self.linear3(x))\n","        mu =  self.linear2(x)\n","        z = mu + sigma*self.N.sample(mu.shape)\n","        self.kl = (0.5*sigma**2 + 0.5*mu**2 - torch.log(sigma) - 1/2).sum()\n","        return z\n"]},{"cell_type":"markdown","metadata":{"id":"pQu-vMti2S0u"},"source":["The autoencoder class changes a single line of code, swappig out an `Encoder` for a `VariationalEncoder`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Si1LfPiQ2S0u"},"outputs":[],"source":["class VariationalAutoencoder(nn.Module):\n","    def __init__(self, latent_dims,ipnut_dim):\n","        super(VariationalAutoencoder, self).__init__()\n","        self.encoder = VariationalEncoder(latent_dims,input_dim)\n","        self.decoder = Decoder(latent_dims,input_dim)\n","\n","    def forward(self, x):\n","        z = self.encoder(x)\n","        return self.decoder(z)\n"]},{"cell_type":"markdown","metadata":{"id":"SklgUeo32S0v"},"source":["In order to train the variational autoencoder, we only need to add the auxillary loss in our training algorithm.\n","\n","The following code is essentially copy-and-pasted from above, with a single term added added to the loss (`autoencoder.encoder.kl`). We have added a hyperparameter $\\lambda$ to weight the KL contribution to the loss!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iDFRvbht2S0v"},"outputs":[],"source":["def train(autoencoder, data, epochs=20,l=5):\n","    opt = torch.optim.Adam(autoencoder.parameters())\n","    losses_recon = np.array([])\n","    losses_kl = np.array([])\n","    for epoch in range(epochs):\n","        print(\"epoch: \",epoch)\n","        loss_recon_e=0\n","        loss_kl_e=0\n","        for x, y in data:\n","            x = x.to(device) # GPU\n","            opt.zero_grad()\n","            x_hat = autoencoder(x)\n","            loss_recon = ((x - x_hat)**2).sum()\n","            loss_kl = autoencoder.encoder.kl\n","            loss = loss_recon + l*loss_kl\n","            loss.backward()\n","            loss_recon_e += loss_recon.to('cpu').detach().numpy()\n","            loss_kl_e += loss_kl.to('cpu').detach().numpy()\n","            opt.step()\n","        losses_recon = np.append(losses_recon,loss_recon_e)\n","        losses_kl = np.append(losses_kl,loss_kl_e)\n","    return autoencoder, losses_recon,losses_kl\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data = torch.utils.data.DataLoader(\n","        torchvision.datasets.MNIST('./data',\n","               transform=torchvision.transforms.ToTensor(),\n","               download=True),\n","        batch_size=128,\n","        shuffle=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ee_2aVGK2S0v"},"outputs":[],"source":["latent_dims = 2\n","input_dim=28\n","vae = VariationalAutoencoder(latent_dims,input_dim).to(device) # GPU\n","vae,loss_recon,loss_kl = train(vae, train_data,epochs=40,l=1)\n"]},{"cell_type":"markdown","metadata":{"id":"_hbaJbfu2S0v"},"source":["Let's plot the latent vector representations of a few batches of data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":699},"executionInfo":{"elapsed":5104,"status":"ok","timestamp":1706615820335,"user":{"displayName":"Carl Herrmann","userId":"05516243901234844189"},"user_tz":-60},"id":"5CPEq4nm2S0v","outputId":"dd7418ee-ab24-4592-886b-e11adf7fb2f9"},"outputs":[],"source":["plot_latent(vae, train_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(loss_recon,label='reconstruction error')\n","plt.plot(loss_kl,label='KL divergence')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["> **Question 3**: try different values of $\\lambda$ to see the effect on the latent space!"]},{"cell_type":"markdown","metadata":{"id":"7t6h3fer2S0v"},"source":["We can see that, compared to the traditional autoencoder, the range of values for latent vectors is much smaller, and more centralized. The distribution overall of $p(z \\mid x)$ appears to be much closer to a Gaussian distribution.\n","\n","Let's also look at the reconstructed digits from the latent space:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":835},"executionInfo":{"elapsed":4262,"status":"ok","timestamp":1706615856201,"user":{"displayName":"Carl Herrmann","userId":"05516243901234844189"},"user_tz":-60},"id":"HB08NJQx2S0w","outputId":"35add7a5-111a-4801-b77d-c7a3c3f44a14"},"outputs":[],"source":["plot_reconstructed(vae, r0=(-3, 3), r1=(-3, 3),n=20)\n"]},{"cell_type":"markdown","metadata":{"id":"249L2Ydi2S0w"},"source":["# Conclusions\n","\n","Variational autoencoders produce a latent space $Z$ that is more compact and smooth than that learned by traditional autoencoders. This lets us randomly sample points $z \\sim Z$ and produce corresponding reconstructions $\\hat{x} = h(z)$ that form realistic digits, unlike traditional autoencoders."]},{"cell_type":"markdown","metadata":{"id":"GwIvBR6M2S0w"},"source":["## 5 - Interpolation\n","\n","One final thing that I wanted to try out was **interpolation**. Given two inputs $x_1$ and $x_2$, and their corresponding latent vectors $z_1$ and $z_2$, we can interpolate between them by decoding latent vectors between $x_1$ and $x_2$."]},{"cell_type":"markdown","metadata":{"id":"szL9wdzh2S0w"},"source":["The following code produces a row of images showing the interpolation between digits."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B13pRRkM2S0w"},"outputs":[],"source":["def interpolate(autoencoder, x_1, x_2, n=12):\n","    z_1 = autoencoder.encoder(x_1)\n","    z_2 = autoencoder.encoder(x_2)\n","    z = torch.stack([z_1 + (z_2 - z_1)*t for t in np.linspace(0, 1, n)])\n","    interpolate_list = autoencoder.decoder(z)\n","    interpolate_list = interpolate_list.to('cpu').detach().numpy()\n","\n","    w = 28\n","    img = np.zeros((w, n*w))\n","    for i, x_hat in enumerate(interpolate_list):\n","        img[:, i*w:(i+1)*w] = x_hat.reshape(28, 28)\n","    plt.imshow(img)\n","    plt.xticks([])\n","    plt.yticks([])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"198Znilx2S0w"},"outputs":[],"source":["x, y = next(train_data.__iter__()) # hack to grab a batch\n","x_1 = x[y == 1][1].to(device) # find a 1\n","x_2 = x[y == 0][1].to(device) # find a 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"elapsed":920,"status":"ok","timestamp":1706616388914,"user":{"displayName":"Carl Herrmann","userId":"05516243901234844189"},"user_tz":-60},"id":"-SF9F9NI2S0x","outputId":"bba01336-7062-47e4-b4bc-3f36d1d80ba7"},"outputs":[],"source":["interpolate(vae, x_1, x_2, n=20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"elapsed":819,"status":"ok","timestamp":1706616396005,"user":{"displayName":"Carl Herrmann","userId":"05516243901234844189"},"user_tz":-60},"id":"6bpm6fII2S0x","outputId":"7e294d39-b2a7-45a1-f533-ebbe0e2c45c4"},"outputs":[],"source":["interpolate(autoencoder, x_1, x_2, n=20)\n"]},{"cell_type":"markdown","metadata":{"id":"quq1v1D9ALfO"},"source":["## 6 - Exercises"]},{"cell_type":"markdown","metadata":{},"source":["Try to implement a **denoising autoencoder**; you can use the function below to generate noisy input samples with a gaussian noise. \n","\n","1. modifiy the function to allow adding salt 'n pepper noise to the images\n","2. implement a DAE; you can use different numbers of layers / activation functions / dimension of latent space\n","3. Make a plot with the original image, the noised image and the reconstructed image for several examples of input images.\n","4. compare different types (gaussian / salt n pepper) and levels of noise on the performance of the DAE\n","5. compare different DAE architectures regarding the denoising performances.\n"]},{"cell_type":"markdown","metadata":{},"source":["We start by noising the training and testing images with gaussian noise"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_images = train_loader.dataset.data.detach().numpy()\n","test_images = test_loader.dataset.data.detach().numpy()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_gaussian_noise(img,mean=10,var=30):\n","    import math\n","    img=img.astype(np.float32)\n","\n","    sigma=var**.5\n","    noise=np.random.normal(mean,sigma,img.shape)\n","    img=img+noise\n","    return img\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["noised_train = np.zeros((len(train_images),28,28),dtype='float32')\n","for i in range(len(train_images)):\n","  noised_train[i]=add_gaussian_noise(train_images[i]).reshape(28,28)\n","  noised_train[i] = np.float32(noised_train[i])\n","\n","noised_test = np.zeros((len(test_images),28,28),dtype='float32')\n","for i in range(len(test_images)):\n","  noised_test[i]=add_gaussian_noise(test_images[i]).reshape(28,28)\n","  noised_test[i] = np.float32(noised_test[i])\n"]},{"cell_type":"markdown","metadata":{},"source":["This class generates a Dataset object from the original and noised images:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class noisedDataset():\n","\n","  def __init__(self,datasetnoised,datasetclean,transform):\n","    self.noise=datasetnoised\n","    self.clean=datasetclean\n","    #self.labels=labels\n","    self.transform=transform\n","\n","  def __len__(self):\n","    return len(self.noise)\n","\n","  def __getitem__(self,idx):\n","    xNoise=self.noise[idx]\n","    xClean=self.clean[idx]\n","    #y=self.labels[idx]\n","\n","    if self.transform != None:\n","      xNoise=self.transform(xNoise)\n","      xClean=self.transform(xClean)\n","\n","\n","    return (xNoise,xClean)\n","\n","\n","tsfms=transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","train_data = noisedDataset(noised_train,train_images,tsfms)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data\n"]},{"cell_type":"markdown","metadata":{},"source":["### SOLUTION"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
