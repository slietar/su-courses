{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fabe0b3d-64ba-4560-8b2b-ba61d149085f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Evaluation 9-5-24 Cours Deep-Life TME-solo-2\n",
    "\n",
    "Nom: Liétar\n",
    "\n",
    "Prénom: Simon\n",
    "\n",
    "Numéro Etudiant: 21313900\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0163aa-879e-4ca7-89eb-eddc62ef0f2f",
   "metadata": {},
   "source": [
    "# Consignes\n",
    "\n",
    "## Document à rendre\n",
    "Le Jupyter Notebook est le document à rendre. Il doit être rendu de manière à ce qu'il soit facile à lire, et qu'il soit facile de\n",
    "repérer et de lire vos réponses. Si ce n'est pas le cas, des points vous seront retirés, ceci pouvant aller jusqu'à l'intégralité \n",
    "des points pour la question.\n",
    "    \n",
    "\n",
    "## Documents autorisés\n",
    "\n",
    "Vous n'êtes pas autorisés à utiliser d'autres ressources que celles qui vous sont fournies sur Moodle.\n",
    "Vous ne pouvez pas utiliser Internet autrement que pour accéder à Moodle. Si cette consigne n'est pas respectée,\n",
    "cela signifie automatiquement un zéro pour votre copie. \n",
    "\n",
    "\n",
    "Aidez-vous de la fonction help(***), où *** doit être remplacé par l'objet,la fonction... pour lequel vous souhaitez avoir plus d'informations.\n",
    "\n",
    "## Indication sur les fonctions et classes qui peuvent être utilisées:\n",
    "\n",
    "- torch.nn.functional.relu\n",
    "- torch.sigmoid\n",
    "- nn.Linear\n",
    "- torch.sigmoid\n",
    "- matplotlib.pyplot.scatter\n",
    "- numpy.linspace\n",
    "\n",
    "#### Le bareme est donné à titre indicatif\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af808e-53a0-4cab-9623-20d8563a6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import cm\n",
    "import torch; torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274cc11d-36a8-4ea5-a29b-04f0df62bb5d",
   "metadata": {},
   "source": [
    "# Partie 0: Prétaitement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb1a5f-f725-42f0-9885-1b3d86dca8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data= datasets.MNIST(root=\"data\",train=True,download=True,transform=transforms.ToTensor())\n",
    "test_data = datasets.MNIST(root=\"data\",train=False,download=True,transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d5942-1946-4ecd-9cd6-847eacb65436",
   "metadata": {},
   "source": [
    "### Q1 (1pts)\n",
    "Créez un jeu de données, training_data_small, qui ne contient que 1/6 des données de MNIST train, en vous assurant de respecter plus ou moins\n",
    "les proportions des données par classes de MNIST train. Le jeu de données training_data_small que vous retournerez doit être constitué de\n",
    "tenseurs (torch.Tensor) : c'est-à-dire que les images apparaissent en format torch.Tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b11e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la longueur de training_data_small\n",
    "training_data_small_len = len(training_data) // 6\n",
    "\n",
    "# Création d'une permutation pour s'assurer que les proportions des classes soient conservées (en moyenne)\n",
    "permutation = np.random.permutation(len(training_data))[:training_data_small_len]\n",
    "\n",
    "# Filtrage des données\n",
    "training_data_small = [training_data[item_index] for item_index in permutation]\n",
    "\n",
    "# Affichage de la répartition des classes avant et après\n",
    "print(np.bincount(training_data.targets))\n",
    "print(np.bincount([cl for _, cl in training_data_small]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5203a-9b99-4d18-80cf-de32202e6a80",
   "metadata": {},
   "source": [
    "# Partie I: Autoencodeur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6356ca-9a79-4289-bdc6-2ef08c65e45b",
   "metadata": {},
   "source": [
    "### Q2 (3pts)\n",
    "Implémentez un auto-encodeur dont le latent sera de taille 2 et qui sera constitué de deux blocs: \n",
    "\n",
    "- un encodeur constitué de :\n",
    "  - un layer linéaire de taille la dimension d'entrée et de sortie 128 avec une fonction d'activation ReLU\n",
    "  - un layer linéaire de taille 128 en entrée avec une fonction d'activation ReLU\n",
    "  \n",
    "- un décodeur constitué de :\n",
    "  - un layer linéaire de taille de sortie 128 avec une fonction d'activation ReLU\n",
    "  - un layer linéaire de taille d'entrée 128 avec une activation sigmoide\n",
    "\n",
    "Dans la description précédente, toutes les tailles ne sont pas précisées, c'est à vous de compléter les données manquantes de taille. \n",
    "\n",
    "Le diagramme des opérations du réseau est représenté dans le pdf 'model.gv.pdf' dans le dossier de l'examen. Il peut aussi etre telechargé ici: <a href=\"model.gv.pdf\">Download PDF</a>\n",
    "\n",
    "\n",
    "\n",
    "Si possible, utilisez la commande suivante pour reproduire le graphe de votre réseau et vérifier sa compatibilité avec l'image qui vous a été donnée.\n",
    "```python \n",
    "\n",
    "import torchvision\n",
    "from torchview import draw_graph\n",
    "model_graph = draw_graph(autoencoder, input_size=(1,1,28,28), expand_nested=True)\n",
    "model_graph.visual_graph\n",
    "````\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257a1ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(28 * 28, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 2),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    self.decoder = nn.Sequential(\n",
    "      nn.Linear(2, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 28 * 28),\n",
    "      nn.Sigmoid(),\n",
    "      nn.Unflatten(1, (1, 28, 28)),\n",
    "    )\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    return self.decoder(self.encoder(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a485ed-7bd0-4717-be07-b72b85bdf85d",
   "metadata": {},
   "source": [
    "### Q3 (3pts)\n",
    "Faites des batch de taille 128 et entraînez votre autoencodeur pendant 30 epochs avec comme loss la Mean Square Error. Représentez l'évolution de la loss au cours de l'entraînement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e5e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "device = 'mps' if platform.system() == 'Darwin' else 'cpu'\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(training_data_small, batch_size=128, shuffle=True)\n",
    "\n",
    "autoencoder = Autoencoder().to(device)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters())\n",
    "\n",
    "epoch_count = 30\n",
    "losses = np.empty(epoch_count)\n",
    "\n",
    "for epoch in range(epoch_count):\n",
    "  autoencoder.train()\n",
    "  epoch_loss = 0.0\n",
    "\n",
    "  for x, _ in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    xp = x.to(device)\n",
    "\n",
    "    # Calcul de la loss\n",
    "    loss = ((autoencoder(xp) - xp) ** 2).sum()\n",
    "\n",
    "    # Modification des poids\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.cpu().detach().numpy()\n",
    "\n",
    "  losses[epoch] = epoch_loss / len(dataloader.dataset)\n",
    "\n",
    "# Affichage de la loss au cours du temps\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(losses)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3703e1ad-e8cf-4cd4-ae6d-1e7136947e7a",
   "metadata": {},
   "source": [
    "### Q4 (2pts)\n",
    "Affichez une visualisation des représentations des données dans l'espace latent, c'est-à-dire en sortie de l'encodeur. Choisissez une couleur différente par classe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dcb7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  autoencoder.eval()\n",
    "\n",
    "  xs = torch.cat([x for x, _ in training_data_small]).to(device)\n",
    "  ys = [y for _, y in training_data_small]\n",
    "\n",
    "  # Calcul des variables latentes\n",
    "  ls = autoencoder.encoder(xs).cpu()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "im = ax.scatter(ls[:, 0], ls[:, 1], c=ys, s=1.0, cmap='tab10')\n",
    "fig.colorbar(im);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2af5f5-2d02-4289-9969-85915e2ff9f7",
   "metadata": {},
   "source": [
    "### Q5 (2pts)\n",
    "Trouvez un rectangle dans l'espace latent qui contient au moins 90 % des données et découpez ce carré en une grille\n",
    "régulière de taille 10*10 (10 valeurs possibles en abscisse et 10 en ordonnée). Chaque nœud de cette grille \n",
    "est un vecteur de l'espace latent. Affichez les images générées par le décodeur à partir de ces points/nœuds \n",
    "dans le latent. Représentez les images en sortie du décodeur sous la forme d'un tableau facilement compréhensible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c486fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_count = 10\n",
    "\n",
    "# Calcul des points de la grille à intervalle régulier\n",
    "# On utilise le type float32 pour le device MPS qui ne supporte pas float64\n",
    "l0s = np.linspace(ls[:, 0].min(), ls[:, 0].max(), point_count, dtype=np.float32)\n",
    "\n",
    "# On inverse min et max pour que ça corresponde à la visualisation au dessus\n",
    "l1s = np.linspace(ls[:, 1].max(), ls[:, 1].min(), point_count, dtype=np.float32)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "with torch.no_grad():\n",
    "  for col, l0 in enumerate(l0s):\n",
    "    for row, l1 in enumerate(l1s):\n",
    "      x = autoencoder.decoder(torch.tensor([[l0, l1]]).to(device)).cpu().numpy().reshape(28, 28)\n",
    "\n",
    "      ax = fig.add_subplot(point_count, point_count, 1 + col + row * point_count)\n",
    "      ax.imshow(x, cmap='gray')\n",
    "      ax.xaxis.set_visible(False)\n",
    "      ax.yaxis.set_visible(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f51cb1-c4e1-47c9-8e41-424df63b301b",
   "metadata": {},
   "source": [
    "# Partie II: VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b982d-6b5d-4223-bee7-58fa6319f28e",
   "metadata": {},
   "source": [
    "#### Q6 (8pts)\n",
    "\n",
    "1)Implémentez un autoencodeur variationnel dont,\n",
    "\n",
    "- l'encodeur est constitué en ce qui concerne la moyenne mu de :\n",
    "  - un layer linéaire de taille de la dimension d'entrée et de sortie 128 avec une fonction d'activation ReLU\n",
    "  - un layer linéaire de taille 128 en entrée\n",
    "\n",
    "- l'encodeur est constitué en ce qui concerne l'écart type sigma de :\n",
    "  - un layer linéaire de taille de la dimension d'entrée et de sortie 128 avec une fonction d'activation ReLU\n",
    "  - un layer linéaire de taille 128 en entrée suivi d'une fonction exponentielle\n",
    "\n",
    "Le premier layer du réseau pour mu et sigma peut être partagé, c'est-à-dire qu'ils partagent le meme premier layer.\n",
    "\n",
    "Comme dans la Partie I :\n",
    "- le décodeur est constitué de :\n",
    "  - un layer linéaire de taille de sortie 128 avec une fonction d'activation ReLU\n",
    "  - un layer linéaire de taille d'entrée 128 avec une sigmoide\n",
    "\n",
    "\n",
    "Indication : vous pouvez vous aider du document 'rappel.pdf' pour implémenter le VAE.\n",
    "\n",
    "\n",
    "2)Faites des batch de taille 128 et entraînez votre autoencodeur pendant 30 epochs avec comme loss la Mean Square Error. Représentez l'évolution de la loss au cours de l'entraînement. Vous êtes libre de choisir la régularisation que vous souhaitez.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60cad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # Couche partagée entre mu et sigma\n",
    "    self.encoder_shared = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(28 * 28, 128),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.encoder_mu = nn.Sequential(\n",
    "      nn.Linear(128, 2),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.encoder_sigma = nn.Linear(128, 2)\n",
    "\n",
    "    # Même décodeur\n",
    "    self.decoder = nn.Sequential(\n",
    "      nn.Linear(2, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 28 * 28),\n",
    "      nn.Sigmoid(),\n",
    "      nn.Unflatten(1, (1, 28, 28)),\n",
    "    )\n",
    "\n",
    "    self.normal = torch.distributions.Normal(0, 1)\n",
    "    self.normal.loc = self.normal.loc.to(device)\n",
    "    self.normal.scale = self.normal.scale.to(device)\n",
    "\n",
    "    self.kl = 0\n",
    "\n",
    "  def encoder(self, x: torch.Tensor):\n",
    "    # Calcul de la première couche partagée\n",
    "    encoded_shared = self.encoder_shared(x)\n",
    "\n",
    "    # Calcul des valeurs de mu et sigma\n",
    "    sigma = torch.exp(self.encoder_sigma(encoded_shared))\n",
    "    mu =  self.encoder_mu(encoded_shared)\n",
    "\n",
    "    # Échantillonage selon la distribution normale\n",
    "    z = mu + sigma * self.normal.sample(mu.shape)\n",
    "\n",
    "    # Calcul de la KL divergence\n",
    "    self.kl = (-torch.log(sigma) + 0.5 * sigma**2 + 0.5 * mu**2 - 0.5).sum()\n",
    "\n",
    "    return z\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    return self.decoder(self.encoder(x))\n",
    "\n",
    "\n",
    "vae = VariationalAutoencoder().to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters())\n",
    "\n",
    "epoch_count = 30\n",
    "lambda_ = 0.01 # Contribution de la KL divergence dans la loss\n",
    "\n",
    "losses = np.empty(epoch_count)\n",
    "\n",
    "for epoch in range(epoch_count):\n",
    "  vae.train()\n",
    "  epoch_loss = 0.0\n",
    "\n",
    "  for x, _ in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    xp = x.to(device)\n",
    "\n",
    "    # Calcul de la loss due à la mauvaise reconstruction\n",
    "    loss_def = ((vae(xp) - xp) ** 2).sum()\n",
    "\n",
    "    # Calcul de la KL divergence\n",
    "    loss = loss_def + lambda_ * vae.kl\n",
    "\n",
    "    # Modification des poids\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.cpu().detach().numpy()\n",
    "\n",
    "  losses[epoch] = epoch_loss / len(dataloader.dataset)\n",
    "\n",
    "# Affichage de la loss au cours du temps\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(losses)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108219dd-6e2c-4580-9ff8-8127c34e85ca",
   "metadata": {},
   "source": [
    "# Partie III: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf82b4-9366-43f6-8245-bb7784327e25",
   "metadata": {},
   "source": [
    "### Q (3pts)\n",
    "Exploitez les représentations compressées des images données par les encodeurs de la première partie et de la seconde partie (VAE) pour entraîner un perceptron à deux couches, chaque couche étant un réseau linéaire avec ReLU. On veut entraîner seulement ces deux couches supplémentaires sans modifier les poids de l'encodeur. Comparez les accuracy sur le jeu de données test des deux encodeurs.\n",
    "\n",
    "Indicate that some weights can be frozen (no gradient descent over those weights) with the following command\n",
    "\n",
    "```python\n",
    " param.requires_grad = False\n",
    " ````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4656d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataLoader pour les données de test\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=True)\n",
    "\n",
    "def classify(ae):\n",
    "  class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "      self.sequence = nn.Sequential(\n",
    "        nn.Linear(2, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10), # Une sortie par classe\n",
    "        nn.Softmax(dim=1)\n",
    "      )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "      return self.sequence(x)\n",
    "\n",
    "  # Gel des poids de l'autoencodeur\n",
    "  ae.requires_grad_(False)\n",
    "\n",
    "  classifier = Classifier().to(device)\n",
    "  optimizer = torch.optim.Adam(classifier.parameters())\n",
    "\n",
    "  epoch_count = 30\n",
    "\n",
    "  accuracies = np.empty((epoch_count, 2))\n",
    "  losses = np.empty(epoch_count)\n",
    "\n",
    "  for epoch in range(epoch_count):\n",
    "    classifier.train()\n",
    "\n",
    "    epoch_acc_train = 0.0\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      xp = x.to(device)\n",
    "      yp = y.to(device)\n",
    "\n",
    "      # Calcul des variables latentes\n",
    "      xl = ae.encoder(xp)\n",
    "\n",
    "      # Prédiction\n",
    "      y_pred = classifier(xl)\n",
    "\n",
    "      # Calcul des loss et accuracy\n",
    "      acc = (y_pred.argmax(dim=1) == yp).sum()\n",
    "      loss = F.cross_entropy(y_pred, yp)\n",
    "\n",
    "      # Modification des poids\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      epoch_acc_train += acc.cpu().detach().numpy()\n",
    "      epoch_loss += loss.cpu().detach().numpy()\n",
    "\n",
    "    epoch_acc_test = 0.0\n",
    "\n",
    "    # Évalution sur les données de test\n",
    "    with torch.no_grad():\n",
    "      for x, y in test_dataloader:\n",
    "        xp = x.to(device)\n",
    "        yp = y.to(device)\n",
    "\n",
    "        xl = ae.encoder(xp)\n",
    "        y_pred = classifier(xl)\n",
    "\n",
    "        acc_test = (y_pred.argmax(dim=1) == yp).sum()\n",
    "        epoch_acc_test += acc_test.cpu().detach().numpy()\n",
    "\n",
    "    accuracies[epoch, 0] = epoch_acc_train / len(dataloader.dataset)\n",
    "    accuracies[epoch, 1] = epoch_acc_test / len(test_dataloader.dataset)\n",
    "    losses[epoch] = epoch_loss / len(dataloader.dataset)\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "\n",
    "  ax.plot(accuracies[:, 0], label='Training')\n",
    "  ax.plot(accuracies[:, 1], label='Test')\n",
    "\n",
    "  ax.set_xlabel('Epoch')\n",
    "  ax.set_ylabel('Accuracy')\n",
    "\n",
    "  ax.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b755ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(autoencoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(vae)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
